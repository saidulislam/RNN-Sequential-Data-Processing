# RNN (Recurrent Neural Network) - Sequential Data Processing
This project includes my RNN experiments and templates I reuse. Covers everything from Simple RNN (no one uses) to LSTM, GRU, Attention and SPINN.

In real life example, you won't use Simple RNN. LSTM and GRU are more likely to be used. Attention helps the network focus on an important part of the input instead of searching the whole input and trying to find the answer. A tem from Google Brain and the University of Toronto had in fact proved that Attention could beat LSTM and GRU networks However, most of the implementations use LSTM/GRU and Attention together. For natural language related work, Stack-augmented Parser-Interpreter Neural Network (SPINN) is used. This particular type of network handles sequence data with a tree-linke structure in mind.
